2019-09-14 13:59:26,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = aarthi
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /home/aarthi/hadoop/hadoop-2.8.1/etc/hadoop:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M14.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M14.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/contrib/capacity-scheduler/*.jar:/home/aarthi/hadoop/hadoop-2.8.1/contrib/capacity-scheduler/*.jar:/home/aarthi/hadoop/hadoop-2.8.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-09T06:14Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-09-14 13:59:26,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-14 13:59:27,095 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-14 13:59:27,390 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-14 13:59:27,463 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-09-14 13:59:27,463 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-14 13:59:27,468 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-09-14 13:59:27,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2019-09-14 13:59:27,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-14 13:59:27,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-14 13:59:27,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-09-14 13:59:27,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-09-14 13:59:27,573 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-14 13:59:27,583 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-14 13:59:27,590 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-14 13:59:27,598 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-14 13:59:27,600 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-14 13:59:27,600 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-14 13:59:27,600 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-14 13:59:27,615 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 40071
2019-09-14 13:59:27,615 INFO org.mortbay.log: jetty-6.1.26
2019-09-14 13:59:27,762 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:40071
2019-09-14 13:59:27,967 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-09-14 13:59:27,977 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-09-14 13:59:28,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = aarthi
2019-09-14 13:59:28,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-14 13:59:28,317 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-09-14 13:59:28,327 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50090
2019-09-14 13:59:28,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50090
2019-09-14 13:59:28,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-14 13:59:28,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-14 13:59:28,480 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-09-14 13:59:28,493 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-14 13:59:28,498 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50090: starting
2019-09-14 13:59:29,611 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:30,613 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:31,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:32,617 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:33,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:34,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:35,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:36,628 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:37,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:38,632 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:38,635 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 13:59:38,652 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 13:59:44,654 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:45,657 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:46,659 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:47,660 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:48,662 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:49,663 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:50,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:51,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:52,670 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:53,671 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 13:59:53,672 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 13:59:53,672 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 13:59:59,681 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:00,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:01,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:09,697 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:03,698 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:04,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:05,709 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:06,704 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:07,705 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:08,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:08,709 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:00:08,714 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:00:14,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:14,719 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:16,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:17,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:18,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:19,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:20,734 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:14,737 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:15,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:23,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:23,746 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:00:23,749 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:00:29,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:30,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:31,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:32,757 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:33,758 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:34,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:35,761 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:36,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:37,764 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:38,765 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:38,768 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:00:38,769 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:00:44,770 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:45,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:46,773 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:47,776 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:48,777 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:49,780 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:50,780 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:51,782 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:52,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:53,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:00:53,789 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:00:53,792 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:00:59,795 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:00,798 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:01,800 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:09,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:03,805 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:04,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:05,814 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:06,814 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:07,817 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:08,818 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:08,819 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:01:08,825 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:01:14,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:14,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:16,832 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:17,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:18,836 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:19,839 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:20,841 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:14,843 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:15,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:23,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:01:23,854 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:01:23,855 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:01:26,940 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 14: SIGTERM
2019-09-14 14:01:26,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2019-09-14 14:09:19,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = aarthi
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /home/aarthi/hadoop/hadoop-2.8.1/etc/hadoop:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M14.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M14.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/contrib/capacity-scheduler/*.jar:/home/aarthi/hadoop/hadoop-2.8.1/contrib/capacity-scheduler/*.jar:/home/aarthi/hadoop/hadoop-2.8.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-09T06:14Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-09-14 14:09:19,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-14 14:09:20,094 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-14 14:09:20,476 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-14 14:09:20,557 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-09-14 14:09:20,557 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-14 14:09:20,561 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-09-14 14:09:20,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2019-09-14 14:09:20,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-14 14:09:20,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-14 14:09:20,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-09-14 14:09:20,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-09-14 14:09:20,669 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-14 14:09:20,681 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-14 14:09:20,686 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-14 14:09:20,696 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-14 14:09:20,698 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-14 14:09:20,698 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-14 14:09:20,698 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-14 14:09:20,720 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 35595
2019-09-14 14:09:20,720 INFO org.mortbay.log: jetty-6.1.26
2019-09-14 14:09:20,857 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35595
2019-09-14 14:09:14,075 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-09-14 14:09:14,085 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-09-14 14:09:14,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = aarthi
2019-09-14 14:09:14,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-14 14:09:14,406 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-09-14 14:09:14,416 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50090
2019-09-14 14:09:14,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50090
2019-09-14 14:09:14,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-14 14:09:14,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-14 14:09:14,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-09-14 14:09:14,559 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-14 14:09:14,560 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50090: starting
2019-09-14 14:09:15,663 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:23,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:24,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:25,669 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:26,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:27,675 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:28,683 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:29,685 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:30,685 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:31,687 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:31,690 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:09:31,706 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:09:37,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:38,709 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:39,711 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:40,712 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:41,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:42,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:43,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:44,718 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:45,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:46,724 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:46,725 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:09:46,727 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:09:52,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:53,733 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:54,736 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:55,737 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:56,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:57,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:58,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:09:59,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:00,749 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:01,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:01,754 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:03:01,754 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:03:07,758 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:08,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:09,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:10,765 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:11,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:12,770 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:13,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:14,776 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:14,781 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:16,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:16,787 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:03:16,798 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:03:15,801 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:23,803 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:24,805 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:26,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:27,160 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:28,161 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:29,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:30,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:31,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:32,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:32,167 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:03:32,168 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:03:38,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:39,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:40,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:41,186 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:42,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:43,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:44,192 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:45,194 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:46,194 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:47,198 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:47,200 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:03:47,204 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:03:53,206 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:54,208 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:55,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:56,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:57,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:58,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:03:59,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:00,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:01,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:09,148 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:09,148 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:04:09,149 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:04:08,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:09,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:10,232 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:11,233 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:12,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:13,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:14,240 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:14,242 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:16,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:17,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:17,247 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:04:17,254 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:04:23,257 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:24,259 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:25,260 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:26,265 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:27,267 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:28,269 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:29,271 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:30,272 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:31,273 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:32,275 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:32,276 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:04:32,277 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:04:38,280 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:39,281 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:40,283 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:41,285 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:42,287 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:43,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:44,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:45,298 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:46,300 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:47,309 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:47,303 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:04:47,304 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:04:53,307 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:54,309 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:55,311 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:56,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:57,314 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:58,314 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:04:59,316 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:00,318 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:01,314 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:09,314 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:09,315 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:05:09,325 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:05:08,328 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:09,330 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:10,331 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:11,332 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:12,333 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:13,337 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:14,341 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:14,342 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:16,344 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:17,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:17,346 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:05:17,347 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:05:23,350 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:24,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:25,352 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:26,353 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:27,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:28,357 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:29,359 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:30,361 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:31,363 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:32,364 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:32,366 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:05:32,377 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:05:38,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:39,383 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:40,385 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:41,387 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:42,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:43,390 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:44,392 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:45,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:46,396 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:47,397 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:47,398 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:05:47,405 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:05:53,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:54,414 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:55,417 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:56,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:57,415 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:58,425 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:05:59,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:00,429 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:01,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:09,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:09,433 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:06:09,433 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:06:08,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:09,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:10,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:11,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:12,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:13,448 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:14,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:14,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:16,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:17,457 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:17,458 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:06:17,466 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:06:23,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:24,472 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:25,474 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:26,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:27,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:28,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:29,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:30,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:31,489 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:32,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:32,491 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:06:32,491 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:06:38,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:39,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:40,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:41,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:42,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:43,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:44,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:45,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:46,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:47,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:47,515 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:06:47,524 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:06:53,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:54,553 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:55,555 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:56,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:57,561 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:58,565 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:06:59,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:00,571 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:01,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:09,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:09,577 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:07:09,579 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:07:08,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:09,585 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:10,587 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:11,592 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:12,593 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:13,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:14,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:14,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:16,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:17,607 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:17,608 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:07:17,612 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:07:23,616 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:24,617 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:25,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:26,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:27,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:28,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:29,626 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:30,628 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:31,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:32,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:32,633 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:07:32,636 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:07:38,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:39,641 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:40,642 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:41,646 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:42,647 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:43,648 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:44,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:45,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:46,656 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:47,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:47,661 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:07:47,663 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:07:53,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:54,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:55,669 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:56,671 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:57,675 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:58,676 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:07:59,678 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:08:00,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:08:01,680 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:08:09,683 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:08:09,688 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1442)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:157)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy14.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:145)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:746)
	at java.lang.Thread.run(Thread.java:748)
2019-09-14 14:08:09,690 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-14 14:08:08,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:08:09,698 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:08:10,703 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-14 14:08:11,577 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 14: SIGTERM
2019-09-14 14:08:11,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2019-09-14 14:13:57,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = aarthi
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /home/aarthi/hadoop/hadoop-2.8.1/etc/hadoop:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M14.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M14.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/contrib/capacity-scheduler/*.jar:/home/aarthi/hadoop/hadoop-2.8.1/contrib/capacity-scheduler/*.jar:/home/aarthi/hadoop/hadoop-2.8.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-09T06:14Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-09-14 14:13:57,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-14 14:13:58,346 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-14 14:13:58,690 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-14 14:13:58,771 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-09-14 14:13:58,771 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-14 14:13:58,775 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-09-14 14:13:58,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2019-09-14 14:13:58,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-14 14:13:58,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-14 14:13:58,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-09-14 14:13:58,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-09-14 14:13:58,879 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-14 14:13:58,893 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-14 14:13:58,904 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-14 14:13:58,916 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-14 14:13:58,924 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-14 14:13:58,924 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-14 14:13:58,924 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-14 14:13:58,941 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 40483
2019-09-14 14:13:58,941 INFO org.mortbay.log: jetty-6.1.26
2019-09-14 14:13:59,094 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:40483
2019-09-14 14:13:59,284 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-09-14 14:13:59,292 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-09-14 14:13:59,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = aarthi
2019-09-14 14:13:59,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-14 14:13:59,589 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-09-14 14:13:59,614 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50090
2019-09-14 14:13:59,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50090
2019-09-14 14:13:59,691 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-14 14:13:59,699 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-14 14:13:59,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-09-14 14:13:59,740 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-14 14:13:59,741 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50090: starting
2019-09-14 14:14:00,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-09-14 14:14:00,085 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-09-14 14:14:00,096 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/aarthi/hadoop/datanode/in_use.lock acquired by nodename 14633@ubuntu
2019-09-14 14:14:00,097 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /home/aarthi/hadoop/datanode is not formatted for namespace 1349942537. Formatting...
2019-09-14 14:14:00,098 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-54b25c9c-279a-4637-8e8f-e1bff4264b10 for directory /home/aarthi/hadoop/datanode
2019-09-14 14:14:00,238 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-544149098-127.0.1.1-1450787178184
2019-09-14 14:14:00,238 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184
2019-09-14 14:14:00,238 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184 is not formatted for BP-544149098-127.0.1.1-1450787178184. Formatting ...
2019-09-14 14:14:00,238 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-544149098-127.0.1.1-1450787178184 directory /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184/current
2019-09-14 14:14:00,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1349942537;bpid=BP-544149098-127.0.1.1-1450787178184;lv=-57;nsInfo=lv=-63;cid=CID-ff1ced76-8a70-48d3-91f6-f44d143075bd;nsid=1349942537;c=1450787178184;bpid=BP-544149098-127.0.1.1-1450787178184;dnuuid=null
2019-09-14 14:14:00,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID a4f5eaf5-7f62-4b56-8623-652a31fdf82a
2019-09-14 14:14:00,376 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-54b25c9c-279a-4637-8e8f-e1bff4264b10
2019-09-14 14:14:00,377 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/aarthi/hadoop/datanode/current, StorageType: DISK
2019-09-14 14:14:00,380 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-14 14:14:00,401 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-09-14 14:14:00,401 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-544149098-127.0.1.1-1450787178184
2019-09-14 14:14:00,409 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-544149098-127.0.1.1-1450787178184 on volume /home/aarthi/hadoop/datanode/current...
2019-09-14 14:14:00,451 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-544149098-127.0.1.1-1450787178184 on /home/aarthi/hadoop/datanode/current: 42ms
2019-09-14 14:14:00,451 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-544149098-127.0.1.1-1450787178184: 50ms
2019-09-14 14:14:00,456 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-544149098-127.0.1.1-1450787178184 on volume /home/aarthi/hadoop/datanode/current...
2019-09-14 14:14:00,456 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184/current/replicas doesn't exist 
2019-09-14 14:14:00,456 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-544149098-127.0.1.1-1450787178184 on volume /home/aarthi/hadoop/datanode/current: 0ms
2019-09-14 14:14:00,456 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2019-09-14 14:14:00,457 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-544149098-127.0.1.1-1450787178184 on volume /home/aarthi/hadoop/datanode
2019-09-14 14:14:00,458 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/aarthi/hadoop/datanode, DS-54b25c9c-279a-4637-8e8f-e1bff4264b10): finished scanning block pool BP-544149098-127.0.1.1-1450787178184
2019-09-14 14:14:00,487 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 2/14/19 6:17 PM with interval of 14600000ms
2019-09-14 14:14:00,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-544149098-127.0.1.1-1450787178184 (Datanode Uuid a4f5eaf5-7f62-4b56-8623-652a31fdf82a) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-09-14 14:14:00,525 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/aarthi/hadoop/datanode, DS-54b25c9c-279a-4637-8e8f-e1bff4264b10): no suitable block pools found to scan.  Waiting 1814399932 ms.
2019-09-14 14:14:00,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-544149098-127.0.1.1-1450787178184 (Datanode Uuid a4f5eaf5-7f62-4b56-8623-652a31fdf82a) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-09-14 14:14:00,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 14600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-14 14:14:00,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xcc28edd3d833af99,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 8 msec to generate and 63 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-14 14:14:00,742 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-544149098-127.0.1.1-1450787178184
2019-09-14 14:41:05,754 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 14: SIGTERM
2019-09-14 14:41:05,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2019-09-14 14:55:14,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = aarthi
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /home/aarthi/hadoop/hadoop-2.8.1/etc/hadoop:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M14.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M14.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/contrib/capacity-scheduler/*.jar:/home/aarthi/hadoop/hadoop-2.8.1/contrib/capacity-scheduler/*.jar:/home/aarthi/hadoop/hadoop-2.8.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-09T06:14Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-09-14 14:55:14,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-14 14:55:16,089 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-14 14:55:16,687 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-14 14:55:16,847 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-09-14 14:55:16,847 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-14 14:55:16,862 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-09-14 14:55:16,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2019-09-14 14:55:16,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-14 14:55:16,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-14 14:55:16,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-09-14 14:55:16,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-09-14 14:55:17,048 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-14 14:55:17,058 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-14 14:55:17,064 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-14 14:55:17,077 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-14 14:55:17,079 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-14 14:55:17,080 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-14 14:55:17,080 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-14 14:55:17,105 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 37537
2019-09-14 14:55:17,105 INFO org.mortbay.log: jetty-6.1.26
2019-09-14 14:55:17,347 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:37537
2019-09-14 14:55:17,709 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-09-14 14:55:17,720 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-09-14 14:55:18,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = aarthi
2019-09-14 14:55:18,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-14 14:55:18,737 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-09-14 14:55:18,778 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50090
2019-09-14 14:55:18,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50090
2019-09-14 14:55:19,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-14 14:55:19,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-14 14:55:19,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-09-14 14:55:19,176 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-14 14:55:19,177 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50090: starting
2019-09-14 14:55:20,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-09-14 14:55:20,278 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-09-14 14:55:20,315 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/aarthi/hadoop/datanode/in_use.lock acquired by nodename 14996@ubuntu
2019-09-14 14:55:20,832 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-544149098-127.0.1.1-1450787178184
2019-09-14 14:55:20,833 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184
2019-09-14 14:55:20,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1349942537;bpid=BP-544149098-127.0.1.1-1450787178184;lv=-57;nsInfo=lv=-63;cid=CID-ff1ced76-8a70-48d3-91f6-f44d143075bd;nsid=1349942537;c=1450787178184;bpid=BP-544149098-127.0.1.1-1450787178184;dnuuid=a4f5eaf5-7f62-4b56-8623-652a31fdf82a
2019-09-14 14:55:14,409 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-54b25c9c-279a-4637-8e8f-e1bff4264b10
2019-09-14 14:55:14,409 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/aarthi/hadoop/datanode/current, StorageType: DISK
2019-09-14 14:55:14,440 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-14 14:55:14,498 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-09-14 14:55:14,498 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-544149098-127.0.1.1-1450787178184
2019-09-14 14:55:14,508 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-544149098-127.0.1.1-1450787178184 on volume /home/aarthi/hadoop/datanode/current...
2019-09-14 14:55:14,738 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-544149098-127.0.1.1-1450787178184 on /home/aarthi/hadoop/datanode/current: 159ms
2019-09-14 14:55:14,743 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-544149098-127.0.1.1-1450787178184: 245ms
2019-09-14 14:55:14,755 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-544149098-127.0.1.1-1450787178184 on volume /home/aarthi/hadoop/datanode/current...
2019-09-14 14:55:14,755 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184/current/replicas doesn't exist 
2019-09-14 14:55:14,755 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-544149098-127.0.1.1-1450787178184 on volume /home/aarthi/hadoop/datanode/current: 1ms
2019-09-14 14:55:14,756 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 6ms
2019-09-14 14:55:14,995 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/aarthi/hadoop/datanode, DS-54b25c9c-279a-4637-8e8f-e1bff4264b10): no suitable block pools found to scan.  Waiting 1786718462 ms.
2019-09-14 14:55:15,068 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 2/15/19 12:01 AM with interval of 14600000ms
2019-09-14 14:55:15,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-544149098-127.0.1.1-1450787178184 (Datanode Uuid a4f5eaf5-7f62-4b56-8623-652a31fdf82a) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-09-14 14:55:15,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-544149098-127.0.1.1-1450787178184 (Datanode Uuid a4f5eaf5-7f62-4b56-8623-652a31fdf82a) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-09-14 14:55:15,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 14600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-14 14:55:23,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x25f4ef7b2f985ae,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 89 msec to generate and 251 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-14 14:55:23,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-544149098-127.0.1.1-1450787178184
2019-09-14 15:14:59,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-544149098-127.0.1.1-1450787178184:blk_1073741825_1001 src: /127.0.0.1:46980 dest: /127.0.0.1:50010
2019-09-14 15:14:59,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46980, dest: /127.0.0.1:50010, bytes: 7911474, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_757367367_1, offset: 0, srvID: a4f5eaf5-7f62-4b56-8623-652a31fdf82a, blockid: BP-544149098-127.0.1.1-1450787178184:blk_1073741825_1001, duration: 154079066
2019-09-14 15:14:59,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-544149098-127.0.1.1-1450787178184:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
2019-09-14 23:09:25,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-544149098-127.0.1.1-1450787178184:blk_1073741826_1009 src: /127.0.0.1:47124 dest: /127.0.0.1:50010
2019-09-14 23:09:25,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47124, dest: /127.0.0.1:50010, bytes: 301938, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1624635852_1, offset: 0, srvID: a4f5eaf5-7f62-4b56-8623-652a31fdf82a, blockid: BP-544149098-127.0.1.1-1450787178184:blk_1073741826_1009, duration: 42762686
2019-09-14 23:09:25,703 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-544149098-127.0.1.1-1450787178184:blk_1073741826_1009, type=LAST_IN_PIPELINE terminating
2019-09-14 23:09:25,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-544149098-127.0.1.1-1450787178184:blk_1073741827_1003 src: /127.0.0.1:47126 dest: /127.0.0.1:50010
2019-09-14 23:09:25,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47126, dest: /127.0.0.1:50010, bytes: 128, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1624635852_1, offset: 0, srvID: a4f5eaf5-7f62-4b56-8623-652a31fdf82a, blockid: BP-544149098-127.0.1.1-1450787178184:blk_1073741827_1003, duration: 11593892
2019-09-14 23:09:25,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-544149098-127.0.1.1-1450787178184:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating
2019-09-14 23:09:26,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-544149098-127.0.1.1-1450787178184:blk_1073741828_1004 src: /127.0.0.1:47128 dest: /127.0.0.1:50010
2019-09-14 23:09:26,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47128, dest: /127.0.0.1:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1624635852_1, offset: 0, srvID: a4f5eaf5-7f62-4b56-8623-652a31fdf82a, blockid: BP-544149098-127.0.1.1-1450787178184:blk_1073741828_1004, duration: 4796459
2019-09-14 23:09:26,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-544149098-127.0.1.1-1450787178184:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating
2019-09-14 23:09:26,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-544149098-127.0.1.1-1450787178184:blk_1073741829_1005 src: /127.0.0.1:47130 dest: /127.0.0.1:50010
2019-09-14 23:09:26,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47130, dest: /127.0.0.1:50010, bytes: 113397, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1624635852_1, offset: 0, srvID: a4f5eaf5-7f62-4b56-8623-652a31fdf82a, blockid: BP-544149098-127.0.1.1-1450787178184:blk_1073741829_1005, duration: 23709611
2019-09-14 23:09:26,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-544149098-127.0.1.1-1450787178184:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating
2019-09-14 23:09:45,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-544149098-127.0.1.1-1450787178184:blk_1073741830_1006 src: /127.0.0.1:47140 dest: /127.0.0.1:50010
2019-09-14 23:09:45,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47140, dest: /127.0.0.1:50010, bytes: 134652, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_830946747_1, offset: 0, srvID: a4f5eaf5-7f62-4b56-8623-652a31fdf82a, blockid: BP-544149098-127.0.1.1-1450787178184:blk_1073741830_1006, duration: 48350391
2019-09-14 23:09:45,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-544149098-127.0.1.1-1450787178184:blk_1073741830_1006, type=LAST_IN_PIPELINE terminating
2019-09-14 23:09:59,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-544149098-127.0.1.1-1450787178184:blk_1073741831_1007 src: /127.0.0.1:47162 dest: /127.0.0.1:50010
2019-09-14 23:03:09,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-544149098-127.0.1.1-1450787178184:blk_1073741832_1008 src: /127.0.0.1:47172 dest: /127.0.0.1:50010
2019-09-14 23:03:10,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47172, dest: /127.0.0.1:50010, bytes: 5604140, op: HDFS_WRITE, cliID: DFSClient_attempt_1450814961661_0001_r_000000_0_-765967516_1, offset: 0, srvID: a4f5eaf5-7f62-4b56-8623-652a31fdf82a, blockid: BP-544149098-127.0.1.1-1450787178184:blk_1073741832_1008, duration: 474334807
2019-09-14 23:03:10,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-544149098-127.0.1.1-1450787178184:blk_1073741832_1008, type=LAST_IN_PIPELINE terminating
2019-09-14 23:03:10,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47162, dest: /127.0.0.1:50010, bytes: 33908, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_830946747_1, offset: 0, srvID: a4f5eaf5-7f62-4b56-8623-652a31fdf82a, blockid: BP-544149098-127.0.1.1-1450787178184:blk_1073741831_1007, duration: 11161201332
2019-09-14 23:03:10,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-544149098-127.0.1.1-1450787178184:blk_1073741831_1007, type=LAST_IN_PIPELINE terminating
2019-09-14 23:03:10,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-544149098-127.0.1.1-1450787178184:blk_1073741833_1009 src: /127.0.0.1:47176 dest: /127.0.0.1:50010
2019-09-14 23:03:10,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47176, dest: /127.0.0.1:50010, bytes: 351, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_830946747_1, offset: 0, srvID: a4f5eaf5-7f62-4b56-8623-652a31fdf82a, blockid: BP-544149098-127.0.1.1-1450787178184:blk_1073741833_1009, duration: 4941540
2019-09-14 23:03:10,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-544149098-127.0.1.1-1450787178184:blk_1073741833_1009, type=LAST_IN_PIPELINE terminating
2019-09-14 23:03:10,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-544149098-127.0.1.1-1450787178184:blk_1073741834_1010 src: /127.0.0.1:47180 dest: /127.0.0.1:50010
2019-09-14 23:03:10,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47180, dest: /127.0.0.1:50010, bytes: 33908, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_830946747_1, offset: 0, srvID: a4f5eaf5-7f62-4b56-8623-652a31fdf82a, blockid: BP-544149098-127.0.1.1-1450787178184:blk_1073741834_1010, duration: 3251810
2019-09-14 23:03:10,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-544149098-127.0.1.1-1450787178184:blk_1073741834_1010, type=LAST_IN_PIPELINE terminating
2019-09-14 23:03:10,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-544149098-127.0.1.1-1450787178184:blk_1073741835_1011 src: /127.0.0.1:47182 dest: /127.0.0.1:50010
2019-09-14 23:03:10,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47182, dest: /127.0.0.1:50010, bytes: 134652, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_830946747_1, offset: 0, srvID: a4f5eaf5-7f62-4b56-8623-652a31fdf82a, blockid: BP-544149098-127.0.1.1-1450787178184:blk_1073741835_1011, duration: 14873266
2019-09-14 23:03:10,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-544149098-127.0.1.1-1450787178184:blk_1073741835_1011, type=LAST_IN_PIPELINE terminating
2019-09-14 23:03:14,635 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1009 file /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184/current/finalized/subdir0/subdir0/blk_1073741826 for deletion
2019-09-14 23:03:14,640 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184/current/finalized/subdir0/subdir0/blk_1073741827 for deletion
2019-09-14 23:03:14,641 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184/current/finalized/subdir0/subdir0/blk_1073741828 for deletion
2019-09-14 23:03:14,641 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2019-09-14 23:03:14,641 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2019-09-14 23:03:14,641 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184/current/finalized/subdir0/subdir0/blk_1073741831 for deletion
2019-09-14 23:03:14,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-544149098-127.0.1.1-1450787178184 blk_1073741826_1009 file /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184/current/finalized/subdir0/subdir0/blk_1073741826
2019-09-14 23:03:14,648 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-544149098-127.0.1.1-1450787178184 blk_1073741827_1003 file /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184/current/finalized/subdir0/subdir0/blk_1073741827
2019-09-14 23:03:14,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-544149098-127.0.1.1-1450787178184 blk_1073741828_1004 file /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184/current/finalized/subdir0/subdir0/blk_1073741828
2019-09-14 23:03:14,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-544149098-127.0.1.1-1450787178184 blk_1073741829_1005 file /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184/current/finalized/subdir0/subdir0/blk_1073741829
2019-09-14 23:03:14,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-544149098-127.0.1.1-1450787178184 blk_1073741830_1006 file /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184/current/finalized/subdir0/subdir0/blk_1073741830
2019-09-14 23:03:14,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-544149098-127.0.1.1-1450787178184 blk_1073741831_1007 file /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184/current/finalized/subdir0/subdir0/blk_1073741831
2019-09-14 23:14:45,353 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 14: SIGTERM
2019-09-14 23:14:46,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2019-09-15 12:15:45,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = aarthi
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /home/aarthi/hadoop/hadoop-2.8.1/etc/hadoop:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M14.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M14.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/home/aarthi/hadoop/hadoop-2.8.1/contrib/capacity-scheduler/*.jar:/home/aarthi/hadoop/hadoop-2.8.1/contrib/capacity-scheduler/*.jar:/home/aarthi/hadoop/hadoop-2.8.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-09T06:14Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-09-15 12:15:45,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-15 12:15:45,867 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-15 12:15:46,480 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-15 12:15:46,609 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-09-15 12:15:46,609 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-15 12:15:46,625 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-09-15 12:15:46,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2019-09-15 12:15:46,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-15 12:15:46,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-15 12:15:46,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-09-15 12:15:46,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-09-15 12:15:46,789 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-15 12:15:46,809 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-15 12:15:46,807 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-15 12:15:46,819 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-15 12:15:46,814 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-15 12:15:46,814 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-15 12:15:46,814 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-15 12:15:46,843 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 41497
2019-09-15 12:15:46,843 INFO org.mortbay.log: jetty-6.1.26
2019-09-15 12:15:47,082 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41497
2019-09-15 12:15:47,414 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-09-15 12:15:47,426 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-09-15 12:15:48,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = aarthi
2019-09-15 12:15:48,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-15 12:15:48,471 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-09-15 12:15:48,503 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50090
2019-09-15 12:15:48,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50090
2019-09-15 12:15:48,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-15 12:15:48,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-15 12:15:48,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-09-15 12:15:48,927 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-15 12:15:48,928 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50090: starting
2019-09-15 12:15:50,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-09-15 12:15:50,451 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-09-15 12:15:50,479 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/aarthi/hadoop/datanode/in_use.lock acquired by nodename 18476@ubuntu
2019-09-15 12:15:50,867 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-544149098-127.0.1.1-1450787178184
2019-09-15 12:15:50,867 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184
2019-09-15 12:15:50,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1349942537;bpid=BP-544149098-127.0.1.1-1450787178184;lv=-57;nsInfo=lv=-63;cid=CID-ff1ced76-8a70-48d3-91f6-f44d143075bd;nsid=1349942537;c=1450787178184;bpid=BP-544149098-127.0.1.1-1450787178184;dnuuid=a4f5eaf5-7f62-4b56-8623-652a31fdf82a
2019-09-15 12:15:51,266 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-54b25c9c-279a-4637-8e8f-e1bff4264b10
2019-09-15 12:15:51,266 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/aarthi/hadoop/datanode/current, StorageType: DISK
2019-09-15 12:15:51,316 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-15 12:15:51,389 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-09-15 12:22:51,389 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-544149098-127.0.1.1-1450787178184
2019-09-15 12:22:51,406 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-544149098-127.0.1.1-1450787178184 on volume /home/aarthi/hadoop/datanode/current...
2019-09-15 12:22:51,566 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-544149098-127.0.1.1-1450787178184 on /home/aarthi/hadoop/datanode/current: 149ms
2019-09-15 12:22:51,576 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-544149098-127.0.1.1-1450787178184: 186ms
2019-09-15 12:22:51,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-544149098-127.0.1.1-1450787178184 on volume /home/aarthi/hadoop/datanode/current...
2019-09-15 12:22:51,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/aarthi/hadoop/datanode/current/BP-544149098-127.0.1.1-1450787178184/current/replicas doesn't exist 
2019-09-15 12:22:51,627 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-544149098-127.0.1.1-1450787178184 on volume /home/aarthi/hadoop/datanode/current: 32ms
2019-09-15 12:22:51,630 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 39ms
2019-09-15 12:22:51,875 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/aarthi/hadoop/datanode, DS-54b25c9c-279a-4637-8e8f-e1bff4264b10): no suitable block pools found to scan.  Waiting 1734668582 ms.
2019-09-15 12:22:51,895 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 2/15/19 1:14 PM with interval of 14600000ms
2019-09-15 12:22:51,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-544149098-127.0.1.1-1450787178184 (Datanode Uuid a4f5eaf5-7f62-4b56-8623-652a31fdf82a) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-09-15 12:22:52,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-544149098-127.0.1.1-1450787178184 (Datanode Uuid a4f5eaf5-7f62-4b56-8623-652a31fdf82a) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-09-15 12:22:52,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 14600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-15 12:22:52,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x527a2500b4f6cbc1,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 8 msec to generate and 137 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-15 12:22:52,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-544149098-127.0.1.1-1450787178184
2019-09-15 13:00:16,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-544149098-127.0.1.1-1450787178184:blk_1073741836_1012 src: /127.0.0.1:52370 dest: /127.0.0.1:50010
2019-09-15 13:00:17,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:52370, dest: /127.0.0.1:50010, bytes: 5755709, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1941995964_1, offset: 0, srvID: a4f5eaf5-7f62-4b56-8623-652a31fdf82a, blockid: BP-544149098-127.0.1.1-1450787178184:blk_1073741836_1012, duration: 347526671
2019-09-15 13:00:17,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-544149098-127.0.1.1-1450787178184:blk_1073741836_1012, type=LAST_IN_PIPELINE terminating
2019-09-15 13:14:45,397 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-544149098-127.0.1.1-1450787178184 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-09-15 13:29:32,240 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1901ms
No GCs detected
2019-09-15 14:45:09,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x527a2500b4f6cbc2,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 9 msec to generate and 103 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-15 14:45:09,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-544149098-127.0.1.1-1450787178184